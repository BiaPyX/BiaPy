{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcAryclxsQJ5"
      },
      "source": [
        "# **Inference pipeline**\n",
        "___  \n",
        "  \n",
        "In this notebook we show how to apply **inference** with [BiaPy](https://biapyx.github.io/).  \n",
        "\n",
        "**Without any coding**, we explain step by step how to\n",
        "1. **upload a set of test images** with their corresponding labels,\n",
        "2. **apply the model** to the test images, and\n",
        "4. **download the results** to your local machine.\n",
        "\n",
        "**Disclaimer:** the structure of the notebook is heavily inspired in the fantastic [ZeroCostDL4Mic notebooks](https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki).\n",
        "\n",
        "**Contact:** This notebook was created by [Ignacio Arganda-Carreras](mailto:ignacio.arganda@ehu.eus), [Lenka Backová](mailto:lenka.backova@ehu.eus), [Daniel Franco-Barranco](mailto:daniel.franco@dipc.org) and [Ane Paniagua](mailto:anepaniagua@gmail.com). For suggestions, comments, or issues, please reach out to us via email or [create an issue in BiaPy's repository](https://github.com/BiaPyX/BiaPy/issues). Thank you!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG5ClE_HHQaE"
      },
      "source": [
        "## **Expected inputs and outputs**\n",
        "___\n",
        "**Inputs**\n",
        "\n",
        "This notebook expects three folders as input:\n",
        "* **Test raw images**: with the raw images to test the model.\n",
        "* **Test labels**: ground truth of the test images. Depending on the workflow to be used, the inputs will be images or CSV files.\n",
        "* **Output folder**: a path to store the results.\n",
        "\n",
        "**Outputs**\n",
        "\n",
        "Depending on the workflow, the output could be an image, or a csv file per each test sample.\n",
        "\n",
        "<font color='red'><b>Note</b></font>: for testing purposes, you can also run this notebook with the **example datasets provided in 'Manage file(s) source > Option 3'**.\n",
        "\n",
        "**Data structure**\n",
        "\n",
        "To ensure the proper operation of the library the data directory tree should be something like this:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "└── test\n",
        "    ├── raw\n",
        "    │   ├── testing-0001.tif\n",
        "    │   ├── testing-0002.tif\n",
        "    │   ├── . . .\n",
        "    │   └── testing-9999.tif\n",
        "    └── gt\n",
        "        ├── testing_groundtruth-0001.tif\n",
        "        ├── testing_groundtruth-0002.tif\n",
        "        ├── . . .\n",
        "        └── testing_groundtruth-9999.tif\n",
        "```\n",
        "\n",
        "**⚠️ Warning:** Ensure that images and their corresponding ground truth files are sorted in the same way. A common approach is to fill with zeros the image number added to the filenames (as in the example).\n",
        "\n",
        "**Input Format Support**\n",
        "\n",
        "This notebook is compatible with a range of input formats. You can use the following file extensions: `.tif`, `.png`, `.jpg`, `.npy`, `.h5`, `.hdf5` (every extension supported by [scikit-image](https://scikit-image.org/docs/stable/api/skimage.io.html#skimage.io.imread)).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGSj0DrpUJoY"
      },
      "source": [
        "## **Prepare the environment**\n",
        "___\n",
        "\n",
        "Establish connection with Google services. You **must be logged in to Google** to continue.\n",
        "Since this is not Google's own code, you will probably see a message warning you of the dangers of running unfamiliar code. This is completely normal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bj_sbDFTiZ7"
      },
      "source": [
        "\n",
        "## **Check for GPU access**\n",
        "---\n",
        "\n",
        "By default, the session should be using Python 3 and GPU acceleration, but it is possible to ensure that these are set properly by doing the following:\n",
        "\n",
        "Go to **Runtime -> Change the Runtime type**\n",
        "\n",
        "**Runtime type: Python 3** *(Python 3 is programming language in which this program is written)*\n",
        "\n",
        "**Accelerator: GPU** *(Graphics processing unit)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install BiaPy**\n",
        "---"
      ],
      "metadata": {
        "id": "nfPcAXQnGPRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Play to reinstall Colab libraries so they are compatible wiht BiaPy\n",
        "\n",
        "#@markdown ⚠️ **Wait until this cell is fully executed to run the next one** ⚠️\n",
        "#@markdown This will make the notebook restart. Do not worry and continue executing the next cell once this one is finished.\n",
        "\n",
        "# uninstall Colab's numpy\n",
        "!pip uninstall -y numpy --quiet\n",
        "\n",
        "# install BiaPy's compatible libraries\n",
        "!pip install scipy==1.13.* numpy==1.26.* zarr==2.18.* --quiet\n",
        "\n",
        "# Restart the runtime\n",
        "print('Stopping runtime...')\n",
        "exit()\n",
        "print('You can run the next cell now.')\n",
        "\n",
        "#%load_ext autoreload\n",
        "#%autoreload 2\n",
        "#import numpy as np\n",
        "#import scipy\n",
        "#print(scipy.__version__)\n",
        "#print(np.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD9-PmKZqE5y",
        "outputId": "fc97f2ef-35f8-4b8e-fed4-ee9c6525642e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m890.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Stopping runtime...\n",
            "You can run the next cell now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p33UIUUWLm3V",
        "outputId": "4dab9c0f-6c01-4eed-ef98-547d2492a139",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biapy==3.5.12\n",
            "  Downloading biapy-3.5.12-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.1 in /usr/local/lib/python3.11/dist-packages (from biapy==3.5.12) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from biapy==3.5.12) (1.6.1)\n",
            "Requirement already satisfied: pydot>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from biapy==3.5.12) (3.0.4)\n",
            "Collecting yacs>=0.1.8 (from biapy==3.5.12)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from biapy==3.5.12) (4.67.1)\n",
            "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from biapy==3.5.12) (0.25.2)\n",
            "Collecting edt>=2.3.2 (from biapy==3.5.12)\n",
            "  Downloading edt-3.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Collecting fill-voids>=2.0.6 (from biapy==3.5.12)\n",
            "  Downloading fill_voids-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: opencv-python>=4.8.0.76 in /usr/local/lib/python3.11/dist-packages (from biapy==3.5.12) (4.11.0.86)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.11/dist-packages (from biapy==3.5.12) (2.2.2)\n",
            "Collecting torchinfo>=1.8.0 (from biapy==3.5.12)\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting tensorboardX>=2.6.2.2 (from biapy==3.5.12)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: h5py>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from biapy==3.5.12) (3.13.0)\n",
            "Requirement already satisfied: zarr>=2.16.1 in /usr/local/lib/python3.11/dist-packages (from biapy==3.5.12) (2.18.4)\n",
            "Collecting bioimageio.core==0.7.0 (from biapy==3.5.12)\n",
            "  Downloading bioimageio.core-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting imagecodecs>=2024.1.1 (from biapy==3.5.12)\n",
            "  Downloading imagecodecs-2024.12.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from biapy==3.5.12) (1.26.4)\n",
            "Collecting imgaug>=0.4.0 (from biapy==3.5.12)\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pooch>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from biapy==3.5.12) (1.8.2)\n",
            "Collecting diplib>=3.5.1 (from biapy==3.5.12)\n",
            "  Downloading diplib-3.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting pydantic<2.10,>=2.7.0 (from biapy==3.5.12)\n",
            "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xarray==2025.1.* in /usr/local/lib/python3.11/dist-packages (from biapy==3.5.12) (2025.1.2)\n",
            "Collecting bioimageio.spec==0.5.3.5 (from bioimageio.core==0.7.0->biapy==3.5.12)\n",
            "  Downloading bioimageio.spec-0.5.3.5-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: imageio>=2.10 in /usr/local/lib/python3.11/dist-packages (from bioimageio.core==0.7.0->biapy==3.5.12) (2.37.0)\n",
            "Collecting loguru (from bioimageio.core==0.7.0->biapy==3.5.12)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pydantic-settings>=2.5 (from bioimageio.core==0.7.0->biapy==3.5.12)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bioimageio.core==0.7.0->biapy==3.5.12) (2.32.3)\n",
            "Collecting ruyaml (from bioimageio.core==0.7.0->biapy==3.5.12)\n",
            "  Downloading ruyaml-0.91.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from bioimageio.core==0.7.0->biapy==3.5.12) (4.12.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from xarray==2025.1.*->biapy==3.5.12) (24.2)\n",
            "Requirement already satisfied: annotated-types<1,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bioimageio.spec==0.5.3.5->bioimageio.core==0.7.0->biapy==3.5.12) (0.7.0)\n",
            "Collecting email-validator (from bioimageio.spec==0.5.3.5->bioimageio.core==0.7.0->biapy==3.5.12)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from bioimageio.spec==0.5.3.5->bioimageio.core==0.7.0->biapy==3.5.12) (2.8.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from bioimageio.spec==0.5.3.5->bioimageio.core==0.7.0->biapy==3.5.12) (13.9.4)\n",
            "Requirement already satisfied: tifffile>=2020.7.4 in /usr/local/lib/python3.11/dist-packages (from bioimageio.spec==0.5.3.5->bioimageio.core==0.7.0->biapy==3.5.12) (2025.3.13)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from bioimageio.spec==0.5.3.5->bioimageio.core==0.7.0->biapy==3.5.12) (3.21.0)\n",
            "Collecting fastremap (from fill-voids>=2.0.6->biapy==3.5.12)\n",
            "  Downloading fastremap-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->biapy==3.5.12) (1.17.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->biapy==3.5.12) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->biapy==3.5.12) (11.1.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->biapy==3.5.12) (2.0.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.1->biapy==3.5.12) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.1->biapy==3.5.12) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.1->biapy==3.5.12) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.1->biapy==3.5.12) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.1->biapy==3.5.12) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.3->biapy==3.5.12) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.3->biapy==3.5.12) (2025.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.8.1->biapy==3.5.12) (4.3.7)\n",
            "Collecting pydantic-core==2.23.4 (from pydantic<2.10,>=2.7.0->biapy==3.5.12)\n",
            "  Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->biapy==3.5.12) (3.4.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->biapy==3.5.12) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.0->biapy==3.5.12) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.0->biapy==3.5.12) (3.6.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.6.2.2->biapy==3.5.12) (5.29.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from yacs>=0.1.8->biapy==3.5.12) (6.0.2)\n",
            "Requirement already satisfied: asciitree in /usr/local/lib/python3.11/dist-packages (from zarr>=2.16.1->biapy==3.5.12) (0.3.3)\n",
            "Requirement already satisfied: fasteners in /usr/local/lib/python3.11/dist-packages (from zarr>=2.16.1->biapy==3.5.12) (0.19)\n",
            "Requirement already satisfied: numcodecs!=0.14.0,!=0.14.1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from zarr>=2.16.1->biapy==3.5.12) (0.15.1)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.11/dist-packages (from numcodecs!=0.14.0,!=0.14.1,>=0.10.0->zarr>=2.16.1->biapy==3.5.12) (1.2.18)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.5->bioimageio.core==0.7.0->biapy==3.5.12)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bioimageio.core==0.7.0->biapy==3.5.12) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bioimageio.core==0.7.0->biapy==3.5.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bioimageio.core==0.7.0->biapy==3.5.12) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bioimageio.core==0.7.0->biapy==3.5.12) (2025.1.31)\n",
            "Requirement already satisfied: distro>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from ruyaml->bioimageio.core==0.7.0->biapy==3.5.12) (1.9.0)\n",
            "Requirement already satisfied: setuptools>=39.0 in /usr/local/lib/python3.11/dist-packages (from ruyaml->bioimageio.core==0.7.0->biapy==3.5.12) (75.1.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated->numcodecs!=0.14.0,!=0.14.1,>=0.10.0->zarr>=2.16.1->biapy==3.5.12) (1.17.2)\n",
            "Collecting dnspython>=2.0.0 (from email-validator->bioimageio.spec==0.5.3.5->bioimageio.core==0.7.0->biapy==3.5.12)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->bioimageio.spec==0.5.3.5->bioimageio.core==0.7.0->biapy==3.5.12) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->bioimageio.spec==0.5.3.5->bioimageio.core==0.7.0->biapy==3.5.12) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->bioimageio.spec==0.5.3.5->bioimageio.core==0.7.0->biapy==3.5.12) (0.1.2)\n",
            "Downloading biapy-3.5.12-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bioimageio.core-0.7.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bioimageio.spec-0.5.3.5-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diplib-3.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading edt-3.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fill_voids-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imagecodecs-2024.12.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading fastremap-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruyaml-0.91.0-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yacs, torchinfo, tensorboardX, ruyaml, python-dotenv, pydantic-core, loguru, imagecodecs, fastremap, edt, dnspython, diplib, pydantic, fill-voids, email-validator, pydantic-settings, imgaug, bioimageio.spec, bioimageio.core, biapy\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.27.2\n",
            "    Uninstalling pydantic_core-2.27.2:\n",
            "      Successfully uninstalled pydantic_core-2.27.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.6\n",
            "    Uninstalling pydantic-2.10.6:\n",
            "      Successfully uninstalled pydantic-2.10.6\n",
            "Successfully installed biapy-3.5.12 bioimageio.core-0.7.0 bioimageio.spec-0.5.3.5 diplib-3.5.2 dnspython-2.7.0 edt-3.0.0 email-validator-2.2.0 fastremap-1.15.1 fill-voids-2.0.8 imagecodecs-2024.12.30 imgaug-0.4.0 loguru-0.7.3 pydantic-2.9.2 pydantic-core-2.23.4 pydantic-settings-2.8.1 python-dotenv-1.1.0 ruyaml-0.91.0 tensorboardX-2.6.2.2 torchinfo-1.8.0 yacs-0.1.8\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.4.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.4.0%2Bcu118-cp311-cp311-linux_x86_64.whl (857.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.19.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.19.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.4.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.20.5 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.20.5-py3-none-manylinux2014_x86_64.whl (142.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.9/142.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.20.5 nvidia-nvtx-cu11-11.8.86 torch-2.4.0+cu118 torchaudio-2.4.0+cu118 torchvision-0.19.0+cu118 triton-3.0.0\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Collecting pytorch-msssim\n",
            "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting torchmetrics==1.4.* (from torchmetrics[image]==1.4.*)\n",
            "  Downloading torchmetrics-1.4.3-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (24.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (2.4.0+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics==1.4.*->torchmetrics[image]==1.4.*)\n",
            "  Downloading lightning_utilities-0.14.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting torch-fidelity<=0.4.0 (from torchmetrics[image]==1.4.*)\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: torchvision>=0.8 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]==1.4.*) (0.19.0+cu118)\n",
            "Requirement already satisfied: scipy>1.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]==1.4.*) (1.13.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.29.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (75.1.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (3.18.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (11.8.86)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (3.0.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from torch-fidelity<=0.4.0->torchmetrics[image]==1.4.*) (11.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-fidelity<=0.4.0->torchmetrics[image]==1.4.*) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->torchmetrics==1.4.*->torchmetrics[image]==1.4.*) (1.3.0)\n",
            "Downloading torchmetrics-1.4.3-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.5/869.5 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading lightning_utilities-0.14.2-py3-none-any.whl (28 kB)\n",
            "Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-msssim, torch-fidelity\n",
            "Successfully installed lightning-utilities-0.14.2 pytorch-msssim-1.0.0 torch-fidelity-0.3.0 torchmetrics-1.4.3\n"
          ]
        }
      ],
      "source": [
        "#@markdown ##Play to install BiaPy and its dependences\n",
        "\n",
        "!pip install biapy==3.5.12\n",
        "\n",
        "# Then install Pytorch + CUDA 11.8\n",
        "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Finally install some packages that rely on the Pytorch installation\n",
        "!pip install timm pytorch-msssim torchmetrics[image]==1.4.*\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from skimage.io import imread\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Output\n",
        "from biapy import BiaPy\n",
        "\n",
        "changed_source = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZmI9c09OhSo"
      },
      "source": [
        "## **Manage file(s) source**\n",
        "---\n",
        "The input folder can be provided using three different options: by directly uploading the folder (option 1), by using a folder stored in Google Drive (option 2) or by using a few samples of our data (option 3).\n",
        "\n",
        "Depending on the option chosen, different steps will have to be taken, as explained in the following cells.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPksHcHLO0SU"
      },
      "source": [
        "### **Option 1: Upload Files from Your Local Machine**\n",
        "---\n",
        "You will be prompted to upload your files to Colab and they will be stored under `/content/input/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PafWC0U3XYjd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play the cell to upload local files (test raw images)\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "input_dir = '/content/input/test/raw'\n",
        "\n",
        "if os.path.exists(input_dir):\n",
        "    # Ask the user if they want to delete the existing items in the folder\n",
        "    delete_items = ''\n",
        "    while not delete_items in ['y', 'n']:\n",
        "        delete_items = input(\"Do you want to delete the existing items in the folder? (yes[y]/no[n]): \").strip().lower()\n",
        "    if delete_items == 'y':\n",
        "        for delete_root, delete_dirs, delete_files in os.walk(input_dir, topdown=False):\n",
        "            for name in delete_files:\n",
        "                os.unlink(os.path.join(delete_root, name))\n",
        "else:\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(input_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "%cd {input_dir}\n",
        "uploaded = files.upload()\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Tl1qtfeJXYp1"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play the cell to upload local files (test ground truth)\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "input_dir = '/content/input/test/gt'\n",
        "\n",
        "if os.path.exists(input_dir):\n",
        "    # Ask the user if they want to delete the existing items in the folder\n",
        "    delete_items = ''\n",
        "    while not delete_items in ['y', 'n']:\n",
        "        delete_items = input(\"Do you want to delete the existing items in the folder? (yes[y]/no[n]): \").strip().lower()\n",
        "    if delete_items == 'y':\n",
        "        for delete_root, delete_dirs, delete_files in os.walk(input_dir, topdown=False):\n",
        "            for name in delete_files:\n",
        "                os.unlink(os.path.join(delete_root, name))\n",
        "else:\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(input_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "%cd {input_dir}\n",
        "uploaded = files.upload()\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlwUw2G60_XR"
      },
      "outputs": [],
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##Play the cell to upload local files (YAML file)\n",
        "\n",
        "from google.colab import files\n",
        "%cd /content\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaZUyZyn0_XR"
      },
      "outputs": [],
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##Play the cell to upload local files (model's weights)\n",
        "\n",
        "from google.colab import files\n",
        "%cd /content\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLXGd55gUYjK"
      },
      "source": [
        "### **Option 2: Mount Your Google Drive**\n",
        "---\n",
        "To use this notebook on your own data from Google Drive, you need to mount Google Drive first.\n",
        "\n",
        "Play the cell below to mount your Google Drive and follow the link that will be shown. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive.\n",
        "\n",
        "Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-yXrZLdUk3Z",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
        "\n",
        "#@markdown * Click on the URL.\n",
        "\n",
        "#@markdown * Sign in your Google Account.\n",
        "\n",
        "#@markdown * Copy the authorization code.\n",
        "\n",
        "#@markdown * Enter the authorization code.\n",
        "\n",
        "#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\".\n",
        "\n",
        "# mount user's Google Drive to Google Colab.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEv7FBXFQvjv"
      },
      "source": [
        "## **Paths for Input Images and Output Files**\n",
        "___\n",
        "\n",
        "Depending on the option you chose for managing file sources, you'll set your paths differently:\n",
        "\n",
        "- **Option 1 (Upload from Local Machine)**:\n",
        "  - Set `test_data_path` to `/content/input/test/raw`\n",
        "  - Set `test_data_gt_path` to `/content/input/test/gt`\n",
        "  - Set `yaml_file` to `/content/your_yaml_file.yaml`\n",
        "  - Set `checkpoint_file` to `/content/your_checkpoint.pth`\n",
        "  - Set `output_path` to `/content/out`\n",
        "  \n",
        "- **Option 2 (Use Google Drive Data)**:\n",
        "  - Insert the paths to your input files and your desired output directory here, i.e., `/content/gdrive/MyDrive/...`.\n",
        "  \n",
        "**Note**: Ensure you download your results from the `/content/out` directory after the process!\n",
        "\n",
        "**Helpful Tip**: If you're unsure about the paths to your folders, look at the top left of this notebook for a small folder icon. Navigate through the directories until you locate your desired folder. Right-click on it and select \"Copy Path\" to copy the folder's path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl4e0UIGYZcx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown #####Path to test images\n",
        "test_data_path = '/content/input/test/raw' #@param {type:\"string\"}\n",
        "#@markdown #####Path to test ground truth (if exists)\n",
        "test_data_gt_path = '/content/input/test/gt' #@param {type:\"string\"}\n",
        "#@markdown #####Path to the YAML configuration file\n",
        "yaml_file = '/content/my_2d_semantic_segmentation.yaml' #@param {type:\"string\"}\n",
        "#@markdown #####Path to checkpoint file\n",
        "checkpoint_file = '/content/my_2d_semantic_segmentation_1-checkpoint-best.pth' #@param {type:\"string\"}\n",
        "#@markdown #####Path to store the resulting images (it'll be created if not existing):\n",
        "output_path = '/content/output' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configure and apply your DNN model**\n",
        "---"
      ],
      "metadata": {
        "id": "LcOslR-FGZQa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfUyeHEP4vY3"
      },
      "source": [
        "### **Select your parameters**\n",
        "---\n",
        "* **`load_gt`:** Select to load ground truth labels and measure output performance. **Default value: True**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLdMygZVT5aH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "load_gt = True #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8kLFc8_ajHD"
      },
      "source": [
        "### **Make the inference**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZKK9EoVmH-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f91151a8-8bd3-4ea3-fb79-a6540feff15e",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11:45:38.918311] Inference configuration finished.\n",
            "[11:45:38.929211] Date: 2024-06-18 11:45:38\n",
            "[11:45:38.929341] Arguments: Namespace(config='/content/my_2d_semantic_segmentation.yaml', result_dir='/content/output', name='my_2d_semantic_segmentation', run_id=1, gpu=0, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', dist_backend='nccl')\n",
            "[11:45:38.929389] Job: my_2d_semantic_segmentation_1\n",
            "[11:45:38.929438] Python       : 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "[11:45:38.929491] PyTorch:  2.2.0+cu118\n",
            "[11:45:38.930854] Not using distributed mode\n",
            "[11:45:38.938360] Configuration details:\n",
            "[11:45:38.938444] AUGMENTOR:\n",
            "  AFFINE_MODE: reflect\n",
            "  AUG_NUM_SAMPLES: 10\n",
            "  AUG_SAMPLES: True\n",
            "  BRIGHTNESS: False\n",
            "  BRIGHTNESS_EM: False\n",
            "  BRIGHTNESS_EM_FACTOR: (-0.1, 0.1)\n",
            "  BRIGHTNESS_EM_MODE: 3D\n",
            "  BRIGHTNESS_FACTOR: (-0.1, 0.1)\n",
            "  BRIGHTNESS_MODE: 3D\n",
            "  CBLUR_DOWN_RANGE: (2, 8)\n",
            "  CBLUR_INSIDE: True\n",
            "  CBLUR_SIZE: (0.2, 0.4)\n",
            "  CHANNEL_SHUFFLE: False\n",
            "  CMIX_SIZE: (0.2, 0.4)\n",
            "  CNOISE_NB_ITERATIONS: (1, 3)\n",
            "  CNOISE_SCALE: (0.05, 0.1)\n",
            "  CNOISE_SIZE: (0.2, 0.4)\n",
            "  CONTRAST: False\n",
            "  CONTRAST_EM: False\n",
            "  CONTRAST_EM_FACTOR: (-0.1, 0.1)\n",
            "  CONTRAST_EM_MODE: 3D\n",
            "  CONTRAST_FACTOR: (-0.1, 0.1)\n",
            "  CONTRAST_MODE: 3D\n",
            "  COUT_APPLY_TO_MASK: False\n",
            "  COUT_CVAL: 0.0\n",
            "  COUT_NB_ITERATIONS: (1, 3)\n",
            "  COUT_SIZE: (0.05, 0.3)\n",
            "  CUTBLUR: False\n",
            "  CUTMIX: False\n",
            "  CUTNOISE: False\n",
            "  CUTOUT: False\n",
            "  DA_PROB: 0.5\n",
            "  DRAW_GRID: True\n",
            "  DROPOUT: False\n",
            "  DROP_RANGE: (0, 0.2)\n",
            "  ELASTIC: False\n",
            "  ENABLE: True\n",
            "  E_ALPHA: (12, 16)\n",
            "  E_MODE: constant\n",
            "  E_SIGMA: 4\n",
            "  GAMMA_CONTRAST: False\n",
            "  GAUSSIAN_NOISE: False\n",
            "  GAUSSIAN_NOISE_MEAN: 0.0\n",
            "  GAUSSIAN_NOISE_USE_INPUT_IMG_MEAN_AND_VAR: False\n",
            "  GAUSSIAN_NOISE_VAR: 0.05\n",
            "  GC_GAMMA: (1.25, 1.75)\n",
            "  GRAYSCALE: False\n",
            "  GRIDMASK: False\n",
            "  GRID_D_RANGE: (0.4, 1)\n",
            "  GRID_INVERT: False\n",
            "  GRID_RATIO: 0.6\n",
            "  GRID_ROTATE: 1.0\n",
            "  G_BLUR: False\n",
            "  G_SIGMA: (1.0, 2.0)\n",
            "  HFLIP: True\n",
            "  MB_KERNEL: (3, 7)\n",
            "  MEDIAN_BLUR: False\n",
            "  MISALIGNMENT: False\n",
            "  MISSING_SECTIONS: False\n",
            "  MISSP_ITERATIONS: (10, 30)\n",
            "  MOTB_K_RANGE: (8, 12)\n",
            "  MOTION_BLUR: False\n",
            "  MS_DISPLACEMENT: 16\n",
            "  MS_ROTATE_RATIO: 0.5\n",
            "  PEPPER: False\n",
            "  PEPPER_AMOUNT: 0.05\n",
            "  POISSON_NOISE: False\n",
            "  RANDOM_ROT: True\n",
            "  RANDOM_ROT_RANGE: (-180, 180)\n",
            "  ROT90: False\n",
            "  SALT: False\n",
            "  SALT_AMOUNT: 0.05\n",
            "  SALT_AND_PEPPER: False\n",
            "  SALT_AND_PEPPER_AMOUNT: 0.05\n",
            "  SALT_AND_PEPPER_PROP: 0.5\n",
            "  SHEAR: False\n",
            "  SHEAR_RANGE: (-20, 20)\n",
            "  SHIFT: False\n",
            "  SHIFT_RANGE: (0.1, 0.2)\n",
            "  SHUFFLE_TRAIN_DATA_EACH_EPOCH: True\n",
            "  SHUFFLE_VAL_DATA_EACH_EPOCH: False\n",
            "  VFLIP: True\n",
            "  ZFLIP: False\n",
            "  ZOOM: False\n",
            "  ZOOM_RANGE: (0.8, 1.2)\n",
            "DATA:\n",
            "  CHECK_GENERATORS: False\n",
            "  EXTRACT_RANDOM_PATCH: False\n",
            "  FORCE_RGB: False\n",
            "  NORMALIZATION:\n",
            "    APPLICATION_MODE: image\n",
            "    CUSTOM_MEAN: -1.0\n",
            "    CUSTOM_STD: -1.0\n",
            "    PERC_CLIP: False\n",
            "    PERC_LOWER: -1.0\n",
            "    PERC_UPPER: -1.0\n",
            "    TYPE: div\n",
            "  PATCH_SIZE: (256, 256, 1)\n",
            "  PREPROCESS:\n",
            "    CANNY:\n",
            "      ENABLE: False\n",
            "      HIGH_THRESHOLD: None\n",
            "      LOW_THRESHOLD: None\n",
            "    CLAHE:\n",
            "      CLIP_LIMIT: 0.01\n",
            "      ENABLE: False\n",
            "      KERNEL_SIZE: None\n",
            "    GAUSSIAN_BLUR:\n",
            "      CHANNEL_AXIS: None\n",
            "      ENABLE: False\n",
            "      MODE: nearest\n",
            "      SIGMA: 1\n",
            "    MATCH_HISTOGRAM:\n",
            "      ENABLE: False\n",
            "      REFERENCE_PATH: user_data/test/x\n",
            "    MEDIAN_BLUR:\n",
            "      ENABLE: False\n",
            "    RESIZE:\n",
            "      ANTI_ALIASING: False\n",
            "      CLIP: True\n",
            "      CVAL: 0.0\n",
            "      ENABLE: False\n",
            "      MODE: reflect\n",
            "      ORDER: 1\n",
            "      OUTPUT_SHAPE: (512, 512)\n",
            "      PRESERVE_RANGE: True\n",
            "    TEST: False\n",
            "    TRAIN: False\n",
            "    VAL: False\n",
            "  PROBABILITY_MAP: False\n",
            "  REFLECT_TO_COMPLETE_SHAPE: False\n",
            "  TEST:\n",
            "    ARGMAX_TO_OUTPUT: True\n",
            "    BINARY_MASKS: /content/input/test/x/../bin_mask\n",
            "    CHECK_DATA: True\n",
            "    DETECTION_MASK_DIR: /content/input/test/y_detection_masks\n",
            "    GT_PATH: /content/input/test/y\n",
            "    INSTANCE_CHANNELS_DIR: /content/input/test/x_BC_thick\n",
            "    INSTANCE_CHANNELS_MASK_DIR: /content/input/test/y_BC_thick\n",
            "    IN_MEMORY: True\n",
            "    LOAD_GT: True\n",
            "    MEDIAN_PADDING: False\n",
            "    OVERLAP: (0, 0)\n",
            "    PADDING: (32, 32)\n",
            "    PATH: /content/input/test/x\n",
            "    RESOLUTION: (1, 1)\n",
            "    SSL_SOURCE_DIR: /content/input/test/x_ssl_source\n",
            "    USE_VAL_AS_TEST: False\n",
            "  TRAIN:\n",
            "    CHECK_DATA: True\n",
            "    DETECTION_MASK_DIR: /content/data/train/y_detection_masks\n",
            "    GT_PATH: /content/data/train/y\n",
            "    INPUT_IMG_AXES_ORDER: TZCYX\n",
            "    INPUT_MASK_AXES_ORDER: TZCYX\n",
            "    INPUT_ZARR_MULTIPLE_DATA: False\n",
            "    INPUT_ZARR_MULTIPLE_DATA_GT_PATH: \n",
            "    INPUT_ZARR_MULTIPLE_DATA_RAW_PATH: \n",
            "    INSTANCE_CHANNELS_DIR: /content/data/train/x_BC_thick\n",
            "    INSTANCE_CHANNELS_MASK_DIR: /content/data/train/y_BC_thick\n",
            "    IN_MEMORY: True\n",
            "    MINIMUM_FOREGROUND_PER: 0.01\n",
            "    OVERLAP: (0, 0)\n",
            "    PADDING: (0, 0)\n",
            "    PATH: /content/data/train/x\n",
            "    REPLICATE: 0\n",
            "    RESOLUTION: (1, 1)\n",
            "    SSL_SOURCE_DIR: /content/data/train/x_ssl_source\n",
            "  VAL:\n",
            "    CROSS_VAL: False\n",
            "    CROSS_VAL_FOLD: 1\n",
            "    CROSS_VAL_NFOLD: 5\n",
            "    DETECTION_MASK_DIR: user_data/val/y_detection_masks\n",
            "    DIST_EVAL: True\n",
            "    FROM_TRAIN: True\n",
            "    GT_PATH: user_data/val/y\n",
            "    INPUT_IMG_AXES_ORDER: TZCYX\n",
            "    INPUT_MASK_AXES_ORDER: TZCYX\n",
            "    INPUT_ZARR_MULTIPLE_DATA: False\n",
            "    INPUT_ZARR_MULTIPLE_DATA_GT_PATH: \n",
            "    INPUT_ZARR_MULTIPLE_DATA_RAW_PATH: \n",
            "    INSTANCE_CHANNELS_DIR: user_data/val/x_BC_thick\n",
            "    INSTANCE_CHANNELS_MASK_DIR: user_data/val/y_BC_thick\n",
            "    IN_MEMORY: True\n",
            "    OVERLAP: (0, 0)\n",
            "    PADDING: (0, 0)\n",
            "    PATH: user_data/val/x\n",
            "    RANDOM: True\n",
            "    RESOLUTION: (1, 1)\n",
            "    SPLIT_TRAIN: 0.1\n",
            "    SSL_SOURCE_DIR: user_data/val/x_ssl_source\n",
            "  W_BACKGROUND: 0.06\n",
            "  W_FOREGROUND: 0.94\n",
            "LOG:\n",
            "  CHART_CREATION_FREQ: 5\n",
            "  LOG_DIR: /content/output/my_2d_semantic_segmentation/train_logs\n",
            "  LOG_FILE_PREFIX: my_2d_semantic_segmentation_1\n",
            "  TENSORBOARD_LOG_DIR: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/tensorboard\n",
            "LOSS:\n",
            "  TYPE: CE\n",
            "MODEL:\n",
            "  ACTIVATION: ELU\n",
            "  ARCHITECTURE: unet\n",
            "  BATCH_NORMALIZATION: True\n",
            "  BMZ:\n",
            "    SOURCE_MODEL_DOI: \n",
            "  DROPOUT_VALUES: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  FEATURE_MAPS: [16, 32, 64, 128, 256]\n",
            "  KERNEL_SIZE: 3\n",
            "  LAST_ACTIVATION: sigmoid\n",
            "  LOAD_CHECKPOINT: True\n",
            "  LOAD_CHECKPOINT_EPOCH: best_on_val\n",
            "  LOAD_CHECKPOINT_ONLY_WEIGHTS: True\n",
            "  MAE_DEC_HIDDEN_SIZE: 512\n",
            "  MAE_DEC_MLP_DIMS: 2048\n",
            "  MAE_DEC_NUM_HEADS: 16\n",
            "  MAE_DEC_NUM_LAYERS: 8\n",
            "  MAE_MASK_RATIO: 0.5\n",
            "  MAE_MASK_TYPE: grid\n",
            "  N_CLASSES: 2\n",
            "  SAVE_CKPT_FREQ: -1\n",
            "  SOURCE: biapy\n",
            "  TORCHVISION_MODEL_NAME: \n",
            "  UNETR_DEC_ACTIVATION: relu\n",
            "  UNETR_DEC_KERNEL_SIZE: 3\n",
            "  UNETR_VIT_HIDD_MULT: 3\n",
            "  UNETR_VIT_NUM_FILTERS: 16\n",
            "  UNET_SR_UPSAMPLE_POSITION: pre\n",
            "  UPSAMPLE_LAYER: convtranspose\n",
            "  VIT_EMBED_DIM: 768\n",
            "  VIT_MLP_RATIO: 4.0\n",
            "  VIT_MODEL: custom\n",
            "  VIT_NORM_EPS: 1e-06\n",
            "  VIT_NUM_HEADS: 12\n",
            "  VIT_NUM_LAYERS: 12\n",
            "  VIT_TOKEN_SIZE: 16\n",
            "  Z_DOWN: [2, 2, 2, 2]\n",
            "PATHS:\n",
            "  CHARTS: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/charts\n",
            "  CHECKPOINT: /content/output/my_2d_semantic_segmentation/checkpoints\n",
            "  CHECKPOINT_FILE: /content/my_2d_semantic_segmentation_1-checkpoint-best.pth\n",
            "  DA_SAMPLES: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/aug\n",
            "  GEN_CHECKS: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/gen_check\n",
            "  GEN_MASK_CHECKS: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/gen_mask_check\n",
            "  LWR_X_FILE: /content/output/my_2d_semantic_segmentation/checkpoints/lower_bound_X_perc.npy\n",
            "  LWR_Y_FILE: /content/output/my_2d_semantic_segmentation/checkpoints/lower_bound_Y_perc.npy\n",
            "  MAE_OUT_DIR: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/MAE_checks\n",
            "  MEAN_INFO_FILE: /content/output/my_2d_semantic_segmentation/checkpoints/normalization_mean_value.npy\n",
            "  PROB_MAP_DIR: /content/output/my_2d_semantic_segmentation/prob_map\n",
            "  PROB_MAP_FILENAME: prob_map.npy\n",
            "  PROFILER: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/profiler\n",
            "  RESULT_DIR:\n",
            "    AS_3D_STACK: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/as_3d_stack\n",
            "    AS_3D_STACK_BIN: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/as_3d_stack_binarized\n",
            "    AS_3D_STACK_POST_PROCESSING: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/as_3d_stack_post_processing\n",
            "    DET_ASSOC_POINTS: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/point_associations\n",
            "    DET_LOCAL_MAX_COORDS_CHECK: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/per_image_local_max_check\n",
            "    FULL_IMAGE: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/full_image\n",
            "    FULL_IMAGE_BIN: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/full_image_binarized\n",
            "    FULL_IMAGE_INSTANCES: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/full_image_instances\n",
            "    FULL_IMAGE_POST_PROCESSING: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/full_image_post_processing\n",
            "    INST_ASSOC_POINTS: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/instance_associations\n",
            "    PATH: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1\n",
            "    PER_IMAGE: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/per_image\n",
            "    PER_IMAGE_BIN: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/per_image_binarized\n",
            "    PER_IMAGE_INSTANCES: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/per_image_instances\n",
            "    PER_IMAGE_POST_PROCESSING: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/per_image_post_processing\n",
            "  STD_INFO_FILE: /content/output/my_2d_semantic_segmentation/checkpoints/normalization_std_value.npy\n",
            "  TEST_FULL_GT_H5: /content/input/test/y/h5\n",
            "  TEST_INSTANCE_CHANNELS_CHECK: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/test_BC_instance_channels\n",
            "  TRAIN_INSTANCE_CHANNELS_CHECK: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/train_BC_instance_channels\n",
            "  UPR_X_FILE: /content/output/my_2d_semantic_segmentation/checkpoints/upper_bound_X_perc.npy\n",
            "  UPR_Y_FILE: /content/output/my_2d_semantic_segmentation/checkpoints/upper_bound_Y_perc.npy\n",
            "  VAL_INSTANCE_CHANNELS_CHECK: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/val_BC_instance_channels\n",
            "  WATERSHED_DIR: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/watershed\n",
            "PROBLEM:\n",
            "  DENOISING:\n",
            "    N2V_MANIPULATOR: uniform_withCP\n",
            "    N2V_NEIGHBORHOOD_RADIUS: 5\n",
            "    N2V_PERC_PIX: 0.198\n",
            "    N2V_STRUCTMASK: False\n",
            "  DETECTION:\n",
            "    CENTRAL_POINT_DILATION: 3\n",
            "    CHECK_POINTS_CREATED: True\n",
            "    DATA_CHECK_MW: False\n",
            "  IMAGE_TO_IMAGE:\n",
            "    MULTIPLE_RAW_ONE_TARGET_LOADER: False\n",
            "  INSTANCE_SEG:\n",
            "    DATA_CHANNELS: BC\n",
            "    DATA_CHANNEL_WEIGHTS: (1, 1)\n",
            "    DATA_CHECK_MW: False\n",
            "    DATA_CONTOUR_MODE: thick\n",
            "    DATA_MW_TH_BINARY_MASK: 0.5\n",
            "    DATA_MW_TH_CONTOUR: 0.1\n",
            "    DATA_MW_TH_DISTANCE: 1.0\n",
            "    DATA_MW_TH_FOREGROUND: 0.3\n",
            "    DATA_MW_TH_POINTS: 0.5\n",
            "    DATA_MW_TH_TYPE: auto\n",
            "    DATA_REMOVE_BEFORE_MW: False\n",
            "    DATA_REMOVE_SMALL_OBJ_BEFORE: 10\n",
            "    DISTANCE_CHANNEL_MASK: True\n",
            "    ERODE_AND_DILATE_FOREGROUND: False\n",
            "    FORE_DILATION_RADIUS: 5\n",
            "    FORE_EROSION_RADIUS: 5\n",
            "    SEED_MORPH_RADIUS: []\n",
            "    SEED_MORPH_SEQUENCE: []\n",
            "    WATERSHED_BY_2D_SLICES: False\n",
            "  NDIM: 2D\n",
            "  SELF_SUPERVISED:\n",
            "    NOISE: 0.2\n",
            "    PRETEXT_TASK: crappify\n",
            "    RESIZING_FACTOR: 4\n",
            "  SEMANTIC_SEG:\n",
            "    IGNORE_CLASS_ID: 0\n",
            "  SUPER_RESOLUTION:\n",
            "    UPSCALING: (1, 1)\n",
            "  TYPE: SEMANTIC_SEG\n",
            "SYSTEM:\n",
            "  DEVICE: cpu\n",
            "  NUM_CPUS: 2\n",
            "  NUM_GPUS: 0\n",
            "  NUM_WORKERS: 5\n",
            "  PIN_MEM: True\n",
            "  SEED: 0\n",
            "TEST:\n",
            "  ANALIZE_2D_IMGS_AS_3D_STACK: False\n",
            "  AUGMENTATION: False\n",
            "  BY_CHUNKS:\n",
            "    ENABLE: False\n",
            "    FLUSH_EACH: 100\n",
            "    FORMAT: H5\n",
            "    INPUT_IMG_AXES_ORDER: TZCYX\n",
            "    INPUT_MASK_AXES_ORDER: TZCYX\n",
            "    INPUT_ZARR_MULTIPLE_DATA: False\n",
            "    INPUT_ZARR_MULTIPLE_DATA_GT_PATH: \n",
            "    INPUT_ZARR_MULTIPLE_DATA_RAW_PATH: \n",
            "    SAVE_OUT_TIF: False\n",
            "    WORKFLOW_PROCESS:\n",
            "      ENABLE: True\n",
            "      TYPE: chunk_by_chunk\n",
            "  DET_BLOB_LOG_MAX_SIGMA: 10\n",
            "  DET_BLOB_LOG_MIN_SIGMA: 5\n",
            "  DET_BLOB_LOG_NUM_SIGMA: 2\n",
            "  DET_EXCLUDE_BORDER: False\n",
            "  DET_MIN_TH_TO_BE_PEAK: [0.2]\n",
            "  DET_PEAK_LOCAL_MAX_MIN_DISTANCE: 1\n",
            "  DET_POINT_CREATION_FUNCTION: peak_local_max\n",
            "  DET_TOLERANCE: [10]\n",
            "  ENABLE: True\n",
            "  EVALUATE: True\n",
            "  FULL_IMG: True\n",
            "  MATCHING_STATS: True\n",
            "  MATCHING_STATS_THS: [0.3, 0.5, 0.75]\n",
            "  MATCHING_STATS_THS_COLORED_IMG: [0.3]\n",
            "  POST_PROCESSING:\n",
            "    APPLY_MASK: False\n",
            "    CLEAR_BORDER: False\n",
            "    DET_WATERSHED: False\n",
            "    DET_WATERSHED_DONUTS_CLASSES: [-1]\n",
            "    DET_WATERSHED_DONUTS_NUCLEUS_DIAMETER: 30\n",
            "    DET_WATERSHED_DONUTS_PATCH: [13, 120, 120]\n",
            "    DET_WATERSHED_FIRST_DILATION: [[-1, -1]]\n",
            "    MEASURE_PROPERTIES:\n",
            "      ENABLE: False\n",
            "      REMOVE_BY_PROPERTIES:\n",
            "        ENABLE: False\n",
            "        PROPS: []\n",
            "        SIGN: []\n",
            "        VALUES: []\n",
            "    REMOVE_CLOSE_POINTS: False\n",
            "    REMOVE_CLOSE_POINTS_RADIUS: [-1.0]\n",
            "    REPARE_LARGE_BLOBS_SIZE: -1\n",
            "    VORONOI_ON_MASK: False\n",
            "    VORONOI_TH: 0.0\n",
            "    YZ_FILTERING: False\n",
            "    YZ_FILTERING_SIZE: 5\n",
            "    Z_FILTERING: False\n",
            "    Z_FILTERING_SIZE: 5\n",
            "  REDUCE_MEMORY: False\n",
            "  REUSE_PREDICTIONS: False\n",
            "  VERBOSE: True\n",
            "TRAIN:\n",
            "  ACCUM_ITER: 1\n",
            "  BATCH_SIZE: 6\n",
            "  CHECKPOINT_MONITOR: val_loss\n",
            "  ENABLE: False\n",
            "  EPOCHS: 20\n",
            "  LR: 0.001\n",
            "  LR_SCHEDULER:\n",
            "    MIN_LR: -1.0\n",
            "    NAME: onecycle\n",
            "    REDUCEONPLATEAU_FACTOR: 0.5\n",
            "    REDUCEONPLATEAU_PATIENCE: -1\n",
            "    WARMUP_COSINE_DECAY_EPOCHS: -1\n",
            "  OPTIMIZER: ADAMW\n",
            "  OPT_BETAS: (0.9, 0.999)\n",
            "  PATIENCE: 20\n",
            "  VERBOSE: False\n",
            "  W_DECAY: 0.02\n",
            "[11:45:38.939517] *~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~*\n",
            "[11:45:38.939580] Initializing Semantic_Segmentation_Workflow\n",
            "[11:45:38.942191] *~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~*\n",
            "\n",
            "[11:45:38.944378] Checking ground truth classes in /content/input/test/y . . .\n",
            "[11:45:38.968408] ######################\n",
            "[11:45:38.968589] #   LOAD TEST DATA   #\n",
            "[11:45:38.968634] ######################\n",
            "[11:45:38.968685] 2) Loading test images . . .\n",
            "[11:45:38.968751] Loading data from /content/input/test/x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 690.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11:45:38.983584] *** Loaded data shape is (1, 768, 1024, 1)\n",
            "[11:45:38.983699] 3) Loading test masks . . .\n",
            "[11:45:38.983743] Loading data from /content/input/test/y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 790.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11:45:38.991918] *** Loaded data shape is (1, 768, 1024, 1)\n",
            "[11:45:38.992099] ###############\n",
            "[11:45:38.992145] # Build model #\n",
            "[11:45:38.992176] ###############\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11:45:39.236404] Loading checkpoint from file /content/my_2d_semantic_segmentation_1-checkpoint-best.pth\n",
            "[11:45:39.268745] Model weights loaded!\n",
            "[11:45:39.270463] ############################\n",
            "[11:45:39.271198] #  PREPARE TEST GENERATOR  #\n",
            "[11:45:39.271776] ############################\n",
            "[11:45:39.276430] ###############\n",
            "[11:45:39.277246] #  INFERENCE  #\n",
            "[11:45:39.277846] ###############\n",
            "[11:45:39.278410] Making predictions on test data . . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11:45:39.289100] Processing image: testing-0158.tif\n",
            "[11:45:42.851767] Saving (1, 768, 1024, 1) data as .tif in folder: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/full_image\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.27it/s]\u001b[A\n",
            "                                             \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11:45:43.049115] Saving (1, 768, 1024, 1) data as .tif in folder: /content/output/my_2d_semantic_segmentation/results/my_2d_semantic_segmentation_1/full_image_binarized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:03<00:00,  3.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11:45:43.069417] Releasing memory . . .\n",
            "[11:45:43.069520] #############\n",
            "[11:45:43.069576] #  RESULTS  #\n",
            "[11:45:43.069610] #############\n",
            "[11:45:43.069687] Loss (per image): 1.2073482275009155\n",
            "[11:45:43.069734] Test Foreground IoU (per image): 0.05902527784686678\n",
            "[11:45:43.069770] Test Overall IoU (per image): 0.4835477549131152\n",
            "[11:45:43.069803]  \n",
            "[11:45:43.069878] FINISHED JOB my_2d_semantic_segmentation_1 !!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#@markdown ##Play to run the model\n",
        "import os\n",
        "import errno\n",
        "\n",
        "os.chdir('/content/')\n",
        "\n",
        "# Edit previous configuration file if it exists to load the checkpoint model\n",
        "if os.path.exists( yaml_file ):\n",
        "    import yaml\n",
        "    with open( yaml_file, 'r') as stream:\n",
        "        try:\n",
        "            biapy_config = yaml.safe_load(stream)\n",
        "        except yaml.YAMLError as exc:\n",
        "            print(exc)\n",
        "    biapy_config['PATHS'] = {}\n",
        "    biapy_config['PATHS']['CHECKPOINT_FILE'] = checkpoint_file\n",
        "    biapy_config['MODEL'] = {}\n",
        "    biapy_config['MODEL']['LOAD_CHECKPOINT'] = True\n",
        "\n",
        "    # save file\n",
        "    with open( yaml_file, 'w') as outfile:\n",
        "        yaml.dump(biapy_config, outfile, default_flow_style=False)\n",
        "\n",
        "# Check folders before modifying the .yaml file\n",
        "if not os.path.exists(test_data_path):\n",
        "    raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), test_data_path)\n",
        "ids = sorted(next(os.walk(test_data_path))[2])\n",
        "if len(ids) == 0:\n",
        "    raise ValueError(\"No images found in dir {}\".format(test_data_path))\n",
        "\n",
        "if not os.path.exists(yaml_file):\n",
        "    raise ValueError(\"No YAML configuration file found in {}\".format(yaml_file))\n",
        "\n",
        "if not os.path.exists(checkpoint_file):\n",
        "    raise ValueError(\"No h5 checkpoint file found in {}\".format(checkpoint_file))\n",
        "\n",
        "\n",
        "# open template configuration file\n",
        "import yaml\n",
        "with open( yaml_file, 'r') as stream:\n",
        "    try:\n",
        "        biapy_config = yaml.safe_load(stream)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "\n",
        "biapy_config['DATA']['TEST']['PATH'] = test_data_path\n",
        "biapy_config['DATA']['TEST']['GT_PATH'] = test_data_gt_path\n",
        "\n",
        "biapy_config['DATA']['TEST']['LOAD_GT'] = load_gt\n",
        "biapy_config['TRAIN']['ENABLE'] = False\n",
        "biapy_config['TEST']['ENABLE'] = True\n",
        "biapy_config['MODEL']['LOAD_CHECKPOINT']= True\n",
        "biapy_config['PATHS'] = {}\n",
        "biapy_config['PATHS']['CHECKPOINT_FILE']= checkpoint_file\n",
        "\n",
        "# save file\n",
        "with open( yaml_file, 'w') as outfile:\n",
        "    yaml.dump(biapy_config, outfile, default_flow_style=False)\n",
        "\n",
        "print( \"Inference configuration finished.\")\n",
        "\n",
        "job_name = os.path.splitext(yaml_file)[0].split('/')[-1]\n",
        "\n",
        "# Run the code\n",
        "biapy = BiaPy(yaml_file, result_dir=output_path, name=job_name, run_id=1, gpu=0)\n",
        "biapy.run_job()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdCIYo4ohcAw"
      },
      "source": [
        "## **Download results**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "gnRa9DOUP0FM",
        "outputId": "bbabd2bf-868c-405e-fe52-fd9dbf24a8f0"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_cb5fdf26-9b64-4736-b975-aa1a6e6c0de5\", \"output.zip\", 486782611)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@markdown ###Play to download a zip file with all the results in test.\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "os.chdir('/content/')\n",
        "\n",
        "!zip -r -q /content/output.zip $output_path\n",
        "\n",
        "files.download(\"/content/output.zip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAzTcs0wuR3d"
      },
      "source": [
        "## **Acknowledgments**\n",
        "---\n",
        "We would like to acknowledge the inspiration provided by the excellent [ZeroCostDL4Mic notebooks](https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}