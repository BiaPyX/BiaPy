#!/usr/bin/env python3
import argparse
import importlib
import os
from pathlib import Path
from typing import Callable, Iterable, List, Tuple
from tqdm import tqdm

import numpy as np
import pandas as pd

from biapy.data.data_manipulation import read_img_as_ndarray

def find_label_dirs(root: Path) -> Iterable[Path]:
    """Yield all directories named exactly 'label' under root."""
    for dirpath, dirnames, filenames in os.walk(root):
        # iterate a copy to avoid modifying traversal
        for d in list(dirnames):
            if d == "label":
                yield Path(dirpath) / d


def iter_label_images(label_dir: Path,
                      extensions: Tuple[str, ...]) -> Iterable[Path]:
    """Yield image paths inside a label directory filtered by extension."""
    for name in sorted(os.listdir(label_dir)):
        p = label_dir / name
        if p.is_file() and p.suffix.lower() in extensions:
            yield p


def instances_from_label(label: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    Return (instance_ids, sizes) excluding background 0.
    Uses np.unique for safety with sparse/non-contiguous IDs.
    """
    # squeeze singleton dims (e.g., HxWx1 or 1xHxW from some readers)
    lbl = np.asarray(label)
    lbl = np.squeeze(lbl)

    # ensure integer-ish dtype
    if not np.issubdtype(lbl.dtype, np.integer):
        # if it's float but integer-valued, this will be safe; otherwise it's a data issue
        lbl = lbl.astype(np.int64)

    ids, counts = np.unique(lbl, return_counts=True)
    # drop background id 0 (assumed)
    keep = ids != 0
    return ids[keep], counts[keep]


def main():
    parser = argparse.ArgumentParser(
        description="Collect instance sizes from label images across datasets."
    )
    parser.add_argument("--input-dir", required=True, type=Path,
                        help="Root directory containing datasets (e.g., /datasets)")
    parser.add_argument("--output-dir", required=True, type=Path,
                        help="Directory where the CSV will be saved")
    parser.add_argument("--extensions", type=str, default=".tif,.tiff,.png",
                        help="Comma-separated list of label image extensions to include "
                             "(default: .tif,.tiff,.png)")
    parser.add_argument("--csv-name", type=str, default="instance_sizes.csv",
                        help="CSV filename to write (default: instance_sizes.csv)")
    args = parser.parse_args()

    root: Path = args.input_dir
    outdir: Path = args.output_dir
    outdir.mkdir(parents=True, exist_ok=True)
    csv_path = outdir / args.csv_name
    extensions = tuple(e.strip().lower() for e in args.extensions.split(",") if e.strip())

    rows: List[dict] = []

    # Walk and process
    for label_dir in tqdm(find_label_dirs(root)):
        print("Processing label dir:", label_dir)
        for img_path in tqdm(iter_label_images(label_dir, extensions)):
            is_3d = any([True for x in ["cartocell", "snemi3d", "mitoem"] if x in str(label_dir).lower()])
            label_arr = read_img_as_ndarray(str(img_path), is_3d=is_3d).squeeze()
            inst_ids, sizes = instances_from_label(label_arr)
            # append rows
            for iid, sz in zip(inst_ids, sizes):
                rows.append({
                    "image_path": str(img_path),
                    "instance_id": int(iid),
                    "size": int(sz),
                })

    # Build and save CSV
    if rows:
        df = pd.DataFrame(rows, columns=["image_path", "instance_id", "size"])
    else:
        # still write an empty CSV with headers for pipeline stability
        df = pd.DataFrame(columns=["image_path", "instance_id", "size"])

    df.to_csv(csv_path, index=False)
    print(f"âœ… Wrote {len(df)} rows to {csv_path}")

# import pandas as pd

# # Path to your output CSV
# csv_path = "instance_sizes.csv"

# # Read the CSV
# df = pd.read_csv(csv_path)

# # Extract dataset and split from the path
# df['dataset'] = df['image_path'].str.extract(r'instance_seg_paper/([^/]+)/')[0]
# df['split'] = df['image_path'].str.extract(r'instance_seg_paper/[^/]+/([^/]+)/')[0]

# # Filter only test split
# df_test = df[df['split'].str.contains('test', case=False, na=False)]

# # Compute per-dataset percentile thresholds
# filtered_rows = []
# for dataset, subset in df_test.groupby('dataset'):
#     low = subset['size'].quantile(0.005)   # 0.5 percentile
#     high = subset['size'].quantile(0.998)  # 99.8 percentile
#     filtered = subset[(subset['size'] >= low) & (subset['size'] <= high)]
#     filtered_rows.append(filtered)

# # Concatenate filtered subsets
# df_filtered = pd.concat(filtered_rows, ignore_index=True)

# # Compute min and max per dataset after filtering
# stats = (
#     df_filtered.groupby('dataset')['size']
#     .agg(['min', 'max'])
#     .reset_index()
#     .sort_values('dataset')
# )
# print(stats)
# >>> print(stats)
#      dataset  min      max
# 0  CartoCell  841     5157
# 1   LIVECell   11     5207
# 2     Lizard   11    21862
# 3     MitoEM  220   408336
# 4   Omnipose   32     8837
# 5    SNEMI3D   27  1461877

if __name__ == "__main__":
    main()
